Loading dataset …
  32033 docs  |  vocab size 27

────────────────────────────────────────────────────────────
  Backend: scalar
────────────────────────────────────────────────────────────
  step     1/5000  |  loss 3.3385  |     51.78 ms
  step   500/5000  |  loss 2.4734  |     42.44 ms
  step  1000/5000  |  loss 2.8149  |     33.89 ms
  step  1500/5000  |  loss 2.3399  |     72.70 ms
  step  2000/5000  |  loss 2.1474  |     52.05 ms
  step  2500/5000  |  loss 2.6696  |     44.79 ms
  step  3000/5000  |  loss 2.0654  |     49.80 ms
  step  3500/5000  |  loss 2.1391  |     44.44 ms
  step  4000/5000  |  loss 2.1574  |     85.89 ms
  step  4500/5000  |  loss 2.1669  |     67.28 ms
  step  5000/5000  |  loss 1.9726  |     60.32 ms

  Generated names (10 samples, T=0.7):
    kaleona
    kennn
    awarei
    jarina
    reyir
    rilyn
    aari
    enyle
    kakonn
    sovyn

────────────────────────────────────────────────────────────
  Backend: numpy
────────────────────────────────────────────────────────────
  step     1/5000  |  loss 3.2791  |      0.47 ms
  step   500/5000  |  loss 2.2041  |      0.21 ms
  step  1000/5000  |  loss 2.9137  |      0.21 ms
  step  1500/5000  |  loss 2.4689  |      0.22 ms
  step  2000/5000  |  loss 2.2068  |      0.22 ms
  step  2500/5000  |  loss 2.8935  |      0.21 ms
  step  3000/5000  |  loss 2.0952  |      0.21 ms
  step  3500/5000  |  loss 2.1296  |      0.21 ms
  step  4000/5000  |  loss 1.9622  |      0.23 ms
  step  4500/5000  |  loss 2.3859  |      0.22 ms
  step  5000/5000  |  loss 2.0117  |      0.22 ms

  Generated names (10 samples, T=0.7):
    aurah
    mavilia
    halile
    anayinc
    almen
    cakalle
    manime
    bran
    mawy
    hemirin

────────────────────────────────────────────────────────────
  Backend: torch_cpu
────────────────────────────────────────────────────────────
  step     1/5000  |  loss 3.3029  |    209.99 ms
  step   500/5000  |  loss 2.2707  |     42.84 ms
  step  1000/5000  |  loss 2.8407  |      0.80 ms
  step  1500/5000  |  loss 2.3575  |      2.69 ms
  step  2000/5000  |  loss 2.1047  |      0.56 ms
  step  2500/5000  |  loss 2.5880  |     40.63 ms
  step  3000/5000  |  loss 2.3246  |      0.80 ms
  step  3500/5000  |  loss 1.9970  |      0.58 ms
  step  4000/5000  |  loss 2.2152  |      0.83 ms
  step  4500/5000  |  loss 2.3289  |      0.81 ms
  step  5000/5000  |  loss 1.8685  |      0.81 ms

  Generated names (10 samples, T=0.7):
    horiana
    lhayya
    benie
    alella
    lanad
    ziianse
    ayaneu
    brly
    lodagete
    ganaya

────────────────────────────────────────────────────────────
  Backend: torch_gpu
────────────────────────────────────────────────────────────
  step     1/5000  |  loss 3.2877  |    248.30 ms
  step   500/5000  |  loss 2.3481  |      1.48 ms
  step  1000/5000  |  loss 2.9190  |      1.90 ms
  step  1500/5000  |  loss 2.3128  |      1.48 ms
  step  2000/5000  |  loss 2.1658  |      1.04 ms
  step  2500/5000  |  loss 2.9410  |      1.04 ms
  step  3000/5000  |  loss 2.3586  |      1.04 ms
  step  3500/5000  |  loss 1.8798  |      1.04 ms
  step  4000/5000  |  loss 2.2387  |      1.03 ms
  step  4500/5000  |  loss 2.0135  |      1.04 ms
  step  5000/5000  |  loss 1.9772  |      1.04 ms

  Generated names (10 samples, T=0.7):
    olass
    odelyn
    aleri
    jaron
    kamala
    elyen
    dala
    revie
    khilan
    rianica

=========================================================================
  BENCHMARK SUMMARY
=========================================================================
Backend         Final Loss   Avg(last100)   Total(s)    Median ms   Speedup
-------------------------------------------------------------------------
  scalar        1.9726       2.2843         266.39      51.51       1.0x
  numpy         2.0117       2.3010         1.07        0.21        248.4x
  torch_cpu     1.8685       2.2794         98.20       0.83        2.7x
  torch_gpu     1.9772       2.2842         7.15        1.06        37.3x
=========================================================================

  Speedup relative to scalar baseline:
    numpy          248.4x  ████████████████████████████████████████████████████████████████████████████████
    torch_cpu        2.7x  ██
    torch_gpu       37.3x  █████████████████████████████████████

